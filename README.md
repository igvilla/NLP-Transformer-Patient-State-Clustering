# NLP-Transformer-Patient-State-Clustering
For this project, the main goal was to use free-text responses obtained from chronic pain patients undergoing spinal cord stimulation (SCS) and determine their current patient state. This involved a multi-layered pipeline starting with text pre-processing and followed with a pair of deep learning transformer models, prompt engineering an LLM chat bot, and an unsupervised learning clustering model. 

## Pre-processing and Semantic Search Transformer Model:

Before a clustering model could be applied, the text data had to be converted into numerical data for clustering. The first step in this was to first create a basic algorithm for simple pre-processing steps on the text, mostly focused on sentence separation as this would be vital to the first model used in the feature engineering pipeline. This was followed by implementing a model to detect which of the 7 features of interest (pain, sleep, mood, anxiety, medication, activity, and socialization) are detected in a block of text. These were the seven features that were considered most useful and related to the degree of pain a patient could be feeling. A semantic search sentence transformer was implemented for this step, performing computations on a sentence-by-sentence basis. A question query was designed related to each of the features of interest (e.g. “how much pain is the patient feeling” for the pain feature). A cosine similarity score was then computed for each sentence-query pair to calculate a relevancy score for each feature on that sentence, with a higher score representing that the sentence was more relevant to a certain feature. A high score could be interpreted as the provided input text being a good response that answers the question query appropriately. An optimized threshold of relevancy scores for each feature was determined by maximizing F1 scores for the relevancy/cosine-similarity score results of each feature. This was done by labeling a large set of sentences for the features and optimizing what computed relevancy score could serve as a threshold value for marking each feature as present in the text. With this, one could now determine and mark when a feature of interest is mentioned in a sentence of the patient’s free-text response. 

## Text Classification via Multi-Genre Natural Language Inference Transformer Model:

For each of these features encountered, one now needed to determine more behind the meaning of the feature, such as whether the patient mentioned feeling more pain versus less pain, getting better quality sleep versus struggling to sleep at all, etc. A second transformer model was implemented in this case, a BART Large MNLI model. This model performed natural language inference, computing probabilities of entailment and contradiction that input text has with a certain hypothesis. The probabilities of entailment and contradiction could be converted into class label probabilities as well, therefore performing text classification via this transformer. So, if a hypothesis was constructed regarding the patient feeling more pain, the probabilities of entailment and contradiction are converted into probabilities of two class labels: high degree of pain and low degree of pain. This is done for all the features, with the resulting class label probabilities representing scores for each feature. In our case, this was designed so that high scores (maximum of 1) were correlated to negative connotations behind a patient’s state, such as more pain, less sleep, bad mood, etc. On the other hand, low scores (minimum of 0) were correlated to positive connotations a patient’s state.

## Prompt Engineering Meta Llama 2 Chat Bot:

Although we now had scores for all encountered features, one problem remained: patients tended to give short and vague responses with limited information, on average including at most 3 of the 7 features. Since the BART MNLI model was designed to only compute scores for the features identified by the initial semantic search transformer, this led to a somewhat sparse data set. The initial instinct of imputing the missing feature scores was insufficient since these imputations would be computed on such a sparse dataset with limited statistical information and correlations. Therefore, a better solution had to be created. In this case, a demo chat bot was designed to ask the patient questions regarding any of the missing features. This was a creative method of obtaining more information from the patient to ensure enough data could be used to score each of the seven features. The initial chat bot was designed manually, with some of the limitations being the inability to keep track of chat history and repetitions in the generated responses from the chat bot, resulting in a not-so user friendly and interactive chat bot. 

This was further advanced by implementing the power of the deep pre-trained LLM of Llama 2. This chat bot, which could now generate much more creative and unique responses while also incorporating chat history into the conversation, led to much more advanced chat interactions with users. The chat agent could be defined with detail using prompt engineering, along with several feature-specific prompts. A system prompt was defined to describe the overall task of the AI (obtaining sufficient information of all 7 features to determine their current state). Prompts for each possible missing feature were also designed, prompting the AI to ask more specifically about a certain feature whenever its relevancy score has not yet met the optimized threshold. Whenever the semantic search transformer had not yet identified a feature as relevant in the text, the system prompt and feature-specific prompt were concatenated and fed to generate questions with more emphasis on that specific feature. In addition, since the LLM now maintained record of the chat history, this was designed to always generate a pair of follow-up responses after the patient responds, asking about anything related to the missing features that may have not yet been discussed. After doing so, the relevancy scores would be recalculated to determine whether any other features are still missing, followed by the MNLI transformer recomputing the scores for all discovered features. This process would be repeated until scores were obtained for all 7 features. 

## Clustering Engineered Feature Set:

After doing so, a much larger dataset has been collected, with the scores for each feature now being computed on a much larger set of text, therefore resulting in more reliable and dependable scores. These scores could now finally be fed to a clustering model to determine patient states, which in this case was done by a simple KMeans model (which used an optimal number of clusters via the elbow method and silhouette scores). This resulted in an effective manner of converting free-text responses from a patient into scientifically and mathematically computed determinations of the patient’s current state during treatment. 


